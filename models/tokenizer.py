"""
Main module for the Tokenizer class that is used to encode and decode text data.
"""
import json
import os
import re
import sys

# Add the project directory to the path.
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

from utils.data import SpecialTokens


class Tokenizer:


    # Special methods.
    def __init__(self, vocab_file):
        """
        Initializes the Tokenizer class with the vocabulary file generated by the 
        BPE algorithm.

        Args:
            vocab_file (str): The path to the vocabulary file.
        """
        super().__init__()

        # Check if the file exists.
        if not os.path.isfile(vocab_file):
            raise FileNotFoundError("Vocabulary file not found.")

        # Check if the file is a valid JSON file.
        if not vocab_file.endswith(".json"):
            raise ValueError("Invalid file format. Expected JSON file.")
        
        # Load the vocabulary.
        with open(file=vocab_file, mode="r", encoding="utf-8") as file_buffer:
            self.__text2int = json.load(fp=file_buffer)

        self.__text2int = {token: int(idx) for token, idx in self.__text2int.items()}
        self.__int2text = {idx: token for token, idx in self.__text2int.items()}

        # Attributes.
        self.__vocab_size = len(self.__int2text)
        self.__normalizer = re.compile(pattern=r"[a-z0-9 ']+") # Only lowercase characters and numbers.
        self.__space_replacer = re.compile(pattern=r"\s+")


    def __len__(self):
        """
        Returns the vocabulary size.

        Returns:
            int: The vocabulary size.
        """
        return self.vocab_size


    # Properties.
    @property
    def vocab_size(self):
        """
        Returns the vocabulary size.

        Returns:
            int: The vocabulary size.
        """
        return self.__vocab_size


    # Methods.
    def split_text(self, texts):
        """
        Splits the input text into tokens.

        Args:
            texts (List[str] | str): Text to be splitted by the tokens in the vocabulary.

        Returns:
            List[List[str]]: The splitted text.
        """
        # Check if it is a list of texts.
        if not isinstance(texts, list):
            texts = [texts]

        # Split the text.
        list_of_tokens = []
        for text in texts:

            # Normalize the text.
            text = self.__normalize_text(text=text)

            init_index = 0
            last_index = len(text)
            tokens = []
            while init_index < last_index:

                # Get potential token.
                current_token = text[init_index:last_index]

                # Check if token is in the vocabulary.
                if current_token in self.__text2int:
                    tokens.append(current_token)
                    init_index = last_index
                    last_index = len(text)
                    continue

                # Check if token is a single character.
                if len(current_token) == 1:
                    tokens.append(SpecialTokens.UNK.value[0])
                    init_index += 1
                    last_index = len(text)
                    continue

                last_index -= 1

            list_of_tokens.append(tokens)

        return list_of_tokens


    def encode(self, texts):
        """
        Encodes the input text into integer indices.

        Args:
            texts (List[str] | str): Text to be encoded.

        Returns:
            List[List[int]]: The encoded text.
        """
        # Split the text.
        tokens = self.split_text(texts=texts)

        # Convert tokens into indices.
        indices = []
        for token in tokens:
            indices.append([self.__text2int[tk] for tk in token])

        return indices


    def decode(self, indices):
        """
        Decodes the input indices into text.

        Args:
            indices (List[List[int]] | List[int]): Indices to be decoded.

        Returns:
            List[List[str]]: The decoded text.
        """
        # Check if it is a list of indices.
        if not isinstance(indices, list) or not isinstance(indices[0], list):
            indices = [indices]

        # Decode the indices.
        tokens = []
        for index in indices:
            tokens.append([self.__int2text[idx] for idx in index])

        return tokens


    # Private methods.
    def __normalize_text(self, text):
        """
        Normalizes the input text.

        Args:
            text (str): The text to be normalized.

        Returns:
            str: The normalized text.
        """
        text = text.lower().strip()
        text = self.__normalizer.findall(string=text)
        text = " ".join(text)
        text = self.__space_replacer.sub(repl=" ", string=text)

        return text
