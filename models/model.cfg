# *The embedding_dim must be 768 for all models to match the Bert embeddings 
# size. The Bert model will be used as an Encoder for text data, and this 
# embeddings will be used during cross-attention with the image embeddings.
[model]
    [small]
        image_size = 224
        num_patches = 14
        embedding_dim = 768
        linear_dim = 512
        num_decoder_layers = 4
        encoder_name = "bert-base-uncased"

    [medium]
        image_size = 320
        num_patches = 20
        embedding_dim = 768
        linear_dim = 1024
        num_decoder_layers = 6
        encoder_name = "bert-base-uncased"

    [large]
        image_size = 480
        num_patches = 30
        embedding_dim = 768
        linear_dim = 2048
        num_decoder_layers = 8
        encoder_name = "bert-base-uncased"

[training]
    batch_size = 32
    learning_rate_curve = ""
    learning_rate = 0.00001
    max_iterations = 250000
    eval_interval = 1000
    dropout = 0.0
    weight_decay = 0.0
    exp_dir = "./exp"
    bbox_loss_weight = 5.0
    obj_loss_weight = 2.0
